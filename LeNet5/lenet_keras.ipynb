{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import keras\n",
    "# from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, GlobalAveragePooling2D, BatchNormalization, Activation, AveragePooling2D\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# !pip install kerassurgeon\n",
    "from kerassurgeon import identify \n",
    "from kerassurgeon.operations import delete_channels,delete_layer\n",
    "from kerassurgeon import Surgeon\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_all_conv_layers(model , first_time):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model -> your model\n",
    "        first_time -> type boolean \n",
    "            first_time = True => model is not pruned \n",
    "            first_time = False => model is pruned\n",
    "    Return:\n",
    "        List of Indices containing convolution layers\n",
    "    '''\n",
    "\n",
    "    all_conv_layers = list()\n",
    "    for i,each_layer in enumerate(model.layers):\n",
    "        if (each_layer.name[0:6] == 'conv2d'):\n",
    "            all_conv_layers.append(i)\n",
    "    return all_conv_layers if (first_time==True) else all_conv_layers[1:]\n",
    "\n",
    "\n",
    "def my_get_all_dense_layers(model):\n",
    "    '''\n",
    "    Arguments:\n",
    "        model -> your model        \n",
    "    Return:\n",
    "        List of Indices containing fully connected layers\n",
    "    '''\n",
    "    all_dense_layers = list()\n",
    "    for i,each_layer in enumerate(model.layers):\n",
    "        if (each_layer.name[0:5] == 'dense'):\n",
    "            all_dense_layers.append(i)\n",
    "    return all_dense_layers\n",
    "\n",
    "def my_get_weights_in_conv_layers(model,first_time):\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "        model -> your model\n",
    "        first_time -> boolean variable\n",
    "            first_time = True => model is not pruned \n",
    "            first_time = False => model is pruned\n",
    "    Return:\n",
    "        List containing weight tensors of each layer\n",
    "    '''\n",
    "    weights = list()\n",
    "    all_conv_layers = my_get_all_conv_layers(model,first_time)\n",
    "    layer_wise_weights = list() \n",
    "    for i in all_conv_layers:\n",
    "          weights.append(model.layers[i].get_weights()[0])  \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c74dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_get_l1_norms_filters_per_epoch(weight_list_per_epoch):\n",
    "\n",
    "#     '''\n",
    "#     Arguments:\n",
    "#         List\n",
    "#     Return:\n",
    "#         Number of parmaters, Number of Flops\n",
    "#     '''\n",
    "    \n",
    "#     cosine_similarities_filters_per_epoch = []\n",
    "#     epochs = np.array(weight_list_per_epoch[0]).shape[0]\n",
    "#     for weight_array in weight_list_per_epoch:\n",
    "#         epoch_cosine_similarities = []\n",
    "#         for epochs in weight_array:\n",
    "#             num_filters = epochs.shape[3]\n",
    "#             h, w, d = epochs.shape[0], epochs.shape[1], epochs.shape[2]\n",
    "#             flattened_filters = epochs.reshape(-1, num_filters).T\n",
    "#             cosine_sim = cosine_similarity(flattened_filters)\n",
    "#             summed_cosine_similarities = np.sum(cosine_sim, axis=1) - 1\n",
    "#             epoch_cosine_similarities.append(summed_cosine_similarities.tolist())\n",
    "#         cosine_similarities_filters_per_epoch.append(np.array(epoch_cosine_similarities))\n",
    "\n",
    "#     return cosine_similarities_filters_per_epoch\n",
    "\n",
    "\n",
    "# def my_in_conv_layers_get_sum_of_l1_norms_sorted_indices(weight_list_per_epoch):\n",
    "#     '''\n",
    "#         Arguments:\n",
    "#             weight List \n",
    "#         Return:\n",
    "#             layer_wise_filter_sorted_indices\n",
    "            \n",
    "#     '''\n",
    "#     layer_wise_filter_sorted_indices = list()\n",
    "#     layer_wise_filter_sorted_values = list()\n",
    "#     l1_norms_filters_per_epoch = my_get_l1_norms_filters_per_epoch(weight_list_per_epoch)\n",
    "#     sum_l1_norms = list()\n",
    "    \n",
    "#     for i in l1_norms_filters_per_epoch:\n",
    "#         sum_l1_norms.append(np.sum(i,axis=0))\n",
    "    \n",
    "#     layer_wise_filter_sorted_indices = list()\n",
    "    \n",
    "#     for i in sum_l1_norms:\n",
    "#         a = pd.Series(i).sort_values().index\n",
    "#         layer_wise_filter_sorted_indices.append(a.tolist())\n",
    "#     return layer_wise_filter_sorted_indices\n",
    "\n",
    "\n",
    "# def my_get_percent_prune_filter_indices(layer_wise_filter_sorted_indices,percentage):    \n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         layer_wise_filter_sorted_indices:filters to be \n",
    "#         percentage:percentage of filters to be pruned\n",
    "#     Return:\n",
    "#         prune_filter_indices: indices of filter to be pruned\n",
    "#     \"\"\"\n",
    "#     prune_filter_indices = list()\n",
    "#     for i in range(len(layer_wise_filter_sorted_indices)):\n",
    "#         prune_filter_indices.append(int(len(layer_wise_filter_sorted_indices[i]) * (percentage/100)))\n",
    "#     return prune_filter_indices\n",
    "\n",
    "\n",
    "# def my_get_distance_matrix(l1_norm_matrix):\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         l1_norm_matrix:matrix that stores the l1 norms of filters\n",
    "#     Return:\n",
    "#         distance_matrix: matrix that stores the manhattan distance between filters \n",
    "#     \"\"\"\n",
    "#     distance_matrix = []\n",
    "#     for i,v1 in enumerate(l1_norm_matrix):\n",
    "#         distance_matrix.append([])\n",
    "#         for v2 in l1_norm_matrix:\n",
    "#             distance_matrix[i].append(np.sum((v1-v2)**2))\n",
    "#     return np.array(distance_matrix)\n",
    "    \n",
    "\n",
    "# def my_get_distance_matrix_list(l1_norm_matrix_list):\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         l1_norm_matrix_list:\n",
    "#     Return:\n",
    "#         distance_matrix_list:\n",
    "#     \"\"\" \n",
    "#     distance_matrix_list = []\n",
    "#     for l1_norm_matrix in l1_norm_matrix_list:\n",
    "#         distance_matrix_list.append(my_get_distance_matrix(l1_norm_matrix.T))\n",
    "#     return distance_matrix_list\n",
    "\n",
    "\n",
    "# def my_get_episodes(distance_matrix,percentage):\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         distance_matrix:\n",
    "#         percentage:Percentage of filters to be pruned\n",
    "#     Return:\n",
    "#     episodes:list of filter indices\n",
    "#     \"\"\"\n",
    "#     distance_matrix_flatten = pd.Series(distance_matrix.flatten())\n",
    "#     distance_matrix_flatten = distance_matrix_flatten.sort_values().index.to_list()\n",
    "    \n",
    "#     episodes = list()\n",
    "#     n = distance_matrix.shape[0]\n",
    "#     for i in distance_matrix_flatten:\n",
    "#         episodes.append((i//n,i%n))\n",
    "#     k = int((n * percentage)/100)\n",
    "#     li = list()   \n",
    "#     for i in range(2*k):\n",
    "#         if i%2!=0:\n",
    "#             li.append(episodes[n+i])\n",
    "#     return li\n",
    "\n",
    "\n",
    "# def my_get_episodes_for_all_layers(distance_matrix_list,percentage):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         distance_matrix_list:matrix containing the manhattan distance of all layers\n",
    "#         percentage:percentage of filters to be pruned\n",
    "#     Return:\n",
    "#         all_episodes:all the selected filter pairs\n",
    "#     \"\"\"\n",
    "#     all_episodes = list()\n",
    "#     for matrix in distance_matrix_list:\n",
    "#         all_episodes.append(my_get_episodes(matrix,percentage))\n",
    "#     return all_episodes\n",
    "\n",
    "\n",
    "# def my_get_filter_pruning_indices(episodes_for_all_layers,l1_norm_matrix_list):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         episodes_for_all_layers:list of selected filter pairs \n",
    "#         l1_norm_matrix_list:list of l1 norm matrices of all the filters of each layer\n",
    "#     Return:\n",
    "#         filter_pruning_indices:list of indices of filters to be pruned\n",
    "#     \"\"\"\n",
    "\n",
    "#     filter_pruning_indices = list()\n",
    "#     for layer_index,episode_layer in enumerate(episodes_for_all_layers):\n",
    "#         a = set()\n",
    "#         sum_l1_norms = np.sum(l1_norm_matrix_list[layer_index],axis=0,keepdims=True)\n",
    "\n",
    "#         for episode in episode_layer:\n",
    "#             ep1 = sum_l1_norms.T[episode[0]]\n",
    "#             ep2 = sum_l1_norms.T[episode[1]]\n",
    "#             if ep1 >= ep2:\n",
    "#                 a.add(episode[0])\n",
    "#             else:\n",
    "#                 a.add(episode[1])\n",
    "#             a.add(episode[0])\n",
    "#         a = list(a)\n",
    "#         filter_pruning_indices.append(a)\n",
    "#     return filter_pruning_indices\n",
    "\n",
    "    \n",
    "# def my_delete_filters(model,weight_list_per_epoch,percentage,first_time):\n",
    "#     \"\"\"\n",
    "#     Arguments:\n",
    "#         model:CNN Model\n",
    "#         wieight_list_per_epoch:History\n",
    "#         percentage:Percentage to be pruned\n",
    "#         first_time:Boolean Variable\n",
    "#             first_time -> boolean variable\n",
    "#             first_time = True => model is not pruned \n",
    "#             first_time = False => model is pruned\n",
    "#     Return:\n",
    "#         model_new:input model after pruning\n",
    "\n",
    "#     \"\"\"\n",
    "#     l1_norms = my_get_l1_norms_filters_per_epoch(weight_list_per_epoch)\n",
    "#     distance_matrix_list = my_get_distance_matrix_list(l1_norms)\n",
    "#     episodes_for_all_layers = my_get_episodes_for_all_layers(distance_matrix_list,percentage)\n",
    "#     filter_pruning_indices = my_get_filter_pruning_indices(episodes_for_all_layers,l1_norms)\n",
    "#     all_conv_layers = my_get_all_conv_layers(model,first_time)\n",
    "\n",
    "#     surgeon = Surgeon(model)\n",
    "#     for index,value in enumerate(all_conv_layers):\n",
    "#         print(index,value,filter_pruning_indices[index])\n",
    "#         surgeon.add_job('delete_channels',model.layers[value],channels = filter_pruning_indices[index])\n",
    "#     model_new = surgeon.operate()\n",
    "#     return model_new    \n",
    "\n",
    "def my_get_l1_norms_filters_per_epoch(weight_list_per_epoch):\n",
    "    cosine_similarities_filters_per_epoch = []\n",
    "    epochs = np.array(weight_list_per_epoch[0]).shape[0]\n",
    "    for weight_array in weight_list_per_epoch:\n",
    "        epoch_cosine_similarities = []\n",
    "        for epochs in weight_array:\n",
    "            num_filters = epochs.shape[3]\n",
    "            h, w, d = epochs.shape[0], epochs.shape[1], epochs.shape[2]\n",
    "            flattened_filters = epochs.reshape(-1, num_filters).T\n",
    "            cosine_sim = cosine_similarity(flattened_filters)\n",
    "            summed_cosine_similarities = np.sum(cosine_sim, axis=1) - 1\n",
    "            epoch_cosine_similarities.append(summed_cosine_similarities.tolist())\n",
    "        cosine_similarities_filters_per_epoch.append(np.array(epoch_cosine_similarities))\n",
    "\n",
    "    return cosine_similarities_filters_per_epoch\n",
    "\n",
    "def my_in_conv_layers_get_sum_of_l1_norms_sorted_indices(weight_list_per_epoch):\n",
    "    layer_wise_filter_sorted_indices = list()\n",
    "    l1_norms_filters_per_epoch = my_get_l1_norms_filters_per_epoch(weight_list_per_epoch)\n",
    "    sum_l1_norms = list()\n",
    "    \n",
    "    for i in l1_norms_filters_per_epoch:\n",
    "        sum_l1_norms.append(np.sum(i,axis=0))\n",
    "    \n",
    "    for i in sum_l1_norms:\n",
    "        a = pd.Series(i).sort_values().index\n",
    "        layer_wise_filter_sorted_indices.append(a.tolist())\n",
    "    return layer_wise_filter_sorted_indices\n",
    "\n",
    "def my_get_percent_prune_filter_indices(layer_wise_filter_sorted_indices, percentage):    \n",
    "    prune_filter_indices = list()\n",
    "    for i in range(len(layer_wise_filter_sorted_indices)):\n",
    "        prune_filter_indices.append(int(len(layer_wise_filter_sorted_indices[i]) * (percentage/100)))\n",
    "    return prune_filter_indices\n",
    "\n",
    "def my_get_l1_norm_matrix_list(weight_list_per_epoch):\n",
    "    l1_norm_matrix_list = []\n",
    "    for weight_array in weight_list_per_epoch:\n",
    "        l1_norm_matrix = []\n",
    "        for epoch in weight_array:\n",
    "            l1_norms = np.linalg.norm(epoch.reshape(-1, epoch.shape[-1]), ord=1, axis=0)\n",
    "            l1_norm_matrix.append(l1_norms)\n",
    "        l1_norm_matrix_list.append(np.array(l1_norm_matrix))\n",
    "    return l1_norm_matrix_list\n",
    "\n",
    "def my_get_cosine_similarity_episodes(cosine_similarities, percentage):\n",
    "    num_filters = cosine_similarities.shape[0]\n",
    "    flat_indices = pd.Series(cosine_similarities.flatten()).sort_values().index.to_list()\n",
    "    \n",
    "    episodes = []\n",
    "    for idx in flat_indices:\n",
    "        i, j = divmod(idx, num_filters)\n",
    "        if i != j:\n",
    "            episodes.append((i, j))\n",
    "    \n",
    "    k = int(num_filters * percentage / 100)\n",
    "    return episodes[:2*k:2]  # Select every second pair up to 2*k\n",
    "\n",
    "def my_get_episodes_for_all_layers(l1_norm_matrix_list, percentage):\n",
    "    all_episodes = []\n",
    "    for l1_norm_matrix in l1_norm_matrix_list:\n",
    "        cosine_sim = cosine_similarity(l1_norm_matrix.T)\n",
    "        episodes = my_get_cosine_similarity_episodes(cosine_sim, percentage)\n",
    "        all_episodes.append(episodes)\n",
    "    return all_episodes\n",
    "\n",
    "def my_get_final_pruning_indices(episodes_for_all_layers, l1_norm_matrix_list):\n",
    "    filter_pruning_indices = []\n",
    "    for layer_index, episode_layer in enumerate(episodes_for_all_layers):\n",
    "        a = set()\n",
    "        sum_l1_norms = np.sum(l1_norm_matrix_list[layer_index], axis=0)\n",
    "\n",
    "        for episode in episode_layer:\n",
    "            ep1 = sum_l1_norms[episode[0]]\n",
    "            ep2 = sum_l1_norms[episode[1]]\n",
    "            if ep1 >= ep2:\n",
    "                a.add(episode[0])\n",
    "            else:\n",
    "                a.add(episode[1])\n",
    "        filter_pruning_indices.append(list(a))\n",
    "    return filter_pruning_indices\n",
    "\n",
    "def my_delete_filters(model, weight_list_per_epoch, percentage, first_time):\n",
    "    l1_norm_matrix_list = my_get_l1_norm_matrix_list(weight_list_per_epoch)\n",
    "    episodes_for_all_layers = my_get_episodes_for_all_layers(l1_norm_matrix_list, percentage)\n",
    "    filter_pruning_indices = my_get_final_pruning_indices(episodes_for_all_layers, l1_norm_matrix_list)\n",
    "    all_conv_layers = my_get_all_conv_layers(model, first_time)\n",
    "\n",
    "    surgeon = Surgeon(model)\n",
    "    for index, value in enumerate(all_conv_layers):\n",
    "        print(index, value, filter_pruning_indices[index])\n",
    "        surgeon.add_job('delete_channels', model.layers[value], channels=filter_pruning_indices[index])\n",
    "    model_new = surgeon.operate()\n",
    "    return model_new\n",
    "\n",
    "\n",
    "def count_model_params_flops(model,first_time):\n",
    "\n",
    "    '''\n",
    "    Arguments:\n",
    "        model -> your model\n",
    "        first_time -> boolean variable\n",
    "        first_time = True => model is not pruned \n",
    "        first_time = False => model is pruned\n",
    "    Return:\n",
    "        Number of parmaters, Number of Flops\n",
    "    '''\n",
    "\n",
    "    total_params = 0\n",
    "    total_flops = 0\n",
    "    model_layers = model.layers\n",
    "    for index,layer in enumerate(model_layers):\n",
    "        if any(conv_type in str(type(layer)) for conv_type in ['Conv1D', 'Conv2D', 'Conv3D']):\n",
    "            \n",
    "            params = layer.count_params()\n",
    "            flops = conv_flops(layer)\n",
    "            print(index,layer.name,params,flops)\n",
    "            total_params += params\n",
    "            total_flops += flops\n",
    "        elif 'Dense' in str(type(layer)):\n",
    "            \n",
    "            params = layer.count_params()\n",
    "            flops = dense_flops(layer)\n",
    "            print(index,layer.name,params,flops)\n",
    "            total_params += params\n",
    "            total_flops += flops\n",
    "\n",
    "    return total_params, int(total_flops)\n",
    "\n",
    "\n",
    "def dense_flops(layer):\n",
    "    output_channels = layer.units\n",
    "    input_channels = layer.input_shape[-1]\n",
    "    return 2 * input_channels * output_channels\n",
    "\n",
    "\n",
    "def conv_flops(layer):\n",
    "    output_size = layer.output_shape[1]\n",
    "    kernel_shape = layer.get_weights()[0].shape\n",
    "    return 2 * (output_size ** 2) * (kernel_shape[0] ** 2) * kernel_shape[2] * kernel_shape[3]\n",
    "\n",
    "\n",
    "class Get_Weights(Callback):\n",
    "    def __init__(self,first_time):\n",
    "        super(Get_Weights, self).__init__()\n",
    "        self.weight_list = [] #Using a list of list to store weight tensors per epoch\n",
    "        self.first_time = first_time\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        if epoch == 0:\n",
    "            all_conv_layers = my_get_all_conv_layers(self.model,self.first_time)\n",
    "            for i in range(len(all_conv_layers)):\n",
    "                self.weight_list.append([]) # appending empty lists for later appending weight tensors \n",
    "        \n",
    "        for index,each_weight in enumerate(my_get_weights_in_conv_layers(self.model,self.first_time)):\n",
    "                self.weight_list[index].append(each_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698982ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "###################  Model Building\n",
    "#######################################################################\n",
    "\n",
    "model = keras.Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=20, kernel_size=(5, 5), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(filters=50, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=500, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "def train(model,epochs,first_time):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model:model to be trained\n",
    "        epochs:number of epochs to be trained\n",
    "        first_tim:\n",
    "    Return:\n",
    "        model:trained/fine-tuned Model,\n",
    "        history: accuracies and losses (keras history)\n",
    "        weight_list_per_epoch = all weight tensors per epochs in a list\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    img_rows, img_cols = 28, 28\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    sgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) \n",
    "\n",
    "    gw = Get_Weights(first_time)\n",
    "    history = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=2,\n",
    "            callbacks=[gw],\n",
    "            validation_data=(x_test, y_test))\n",
    "\n",
    "    return model,history,gw.weight_list\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "###################  Model Training\n",
    "#######################################################################\n",
    "\n",
    "model,history,weight_list_per_epoch = train(model,1,True)\n",
    "initial_flops = count_model_params_flops(model,True)[1]\n",
    "log_dict = dict()\n",
    "log_dict['train_loss'] = []\n",
    "log_dict['train_acc'] = []\n",
    "log_dict['val_loss'] = []\n",
    "log_dict['val_acc'] = []\n",
    "log_dict['total_params'] = []\n",
    "log_dict['total_flops'] = []\n",
    "log_dict['filters_in_conv1'] = []\n",
    "log_dict['filters_in_conv2'] = []\n",
    "\n",
    "best_acc_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "log_dict['train_loss'].append(history.history['loss'][best_acc_index])\n",
    "log_dict['train_acc'].append(history.history['accuracy'][best_acc_index])\n",
    "log_dict['val_loss'].append(history.history['val_loss'][best_acc_index])\n",
    "log_dict['val_acc'].append(history.history['val_accuracy'][best_acc_index])\n",
    "a,b = count_model_params_flops(model,True)\n",
    "log_dict['total_params'].append(a)\n",
    "log_dict['total_flops'].append(b)\n",
    "log_dict['filters_in_conv1'].append(model.layers[0].get_weights()[0].shape[-1])\n",
    "log_dict['filters_in_conv2'].append(model.layers[2].get_weights()[0].shape[-1])\n",
    "al = history\n",
    "\n",
    "from keras import backend as K\n",
    "def custom_loss(lmbda , regularizer_value):\n",
    "    def loss(y_true , y_pred):\n",
    "        # print(type(K.categorical_crossentropy(y_true ,y_pred)),K.categorical_crossentropy(y_true ,y_pred),regularizer_value)\n",
    "        return K.categorical_crossentropy(y_true ,y_pred) + lmbda * regularizer_value\n",
    "    return loss\n",
    "\n",
    "def my_get_l1_norms_filters(model,first_time):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model:\n",
    "\n",
    "        first_time : type boolean \n",
    "            first_time = True => model is not pruned \n",
    "            first_time = False => model is pruned\n",
    "        Return:\n",
    "            l1_norms of all filters of every layer as a list\n",
    "    \"\"\"\n",
    "    conv_layers = my_get_all_conv_layers(model, first_time)\n",
    "    cosine_sums = list()\n",
    "    for index, layer_index in enumerate(conv_layers):\n",
    "        cosine_sums.append([])\n",
    "        weights = model.layers[layer_index].get_weights()[0]\n",
    "        num_filters = len(weights[0,0,0,:])\n",
    "        filter_vectors = [weights[:,:,:,i].flatten() for i in range(num_filters)]\n",
    "        \n",
    "        for i in range(num_filters):\n",
    "            similarities = cosine_similarity([filter_vectors[i]], filter_vectors)[0]\n",
    "            cosine_sum = np.sum(similarities) - 1\n",
    "            cosine_sums[index].append(cosine_sum)\n",
    "            \n",
    "    return cosine_sums\n",
    "\n",
    "\n",
    "def my_get_regularizer_value(model,weight_list_per_epoch,percentage,first_time):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model:initial model\n",
    "        weight_list_per_epoch:weight tensors at every epoch\n",
    "        percentage:percentage of filter to be pruned\n",
    "        first_time:type bool\n",
    "    Return:\n",
    "        regularizer_value\n",
    "    \"\"\"\n",
    "    l1_norms_per_epoch = my_get_l1_norms_filters_per_epoch(weight_list_per_epoch)\n",
    "    distance_matrix_list = my_get_distance_matrix_list(l1_norms_per_epoch)\n",
    "    episodes_for_all_layers = my_get_episodes_for_all_layers(distance_matrix_list,percentage)\n",
    "    l1_norms = my_get_l1_norms_filters(model,first_time)\n",
    "    print(episodes_for_all_layers)\n",
    "    regularizer_value = 0\n",
    "    for layer_index,layer in enumerate(episodes_for_all_layers):\n",
    "        for episode in layer:\n",
    "            # print(episode[1],episode[0])\n",
    "            regularizer_value += abs(l1_norms[layer_index][episode[1]] - l1_norms[layer_index][episode[0]])\n",
    "    regularizer_value = np.exp((regularizer_value))\n",
    "    print(regularizer_value)    \n",
    "    return regularizer_value\n",
    "    \n",
    "\n",
    "def optimize(model,weight_list_per_epoch,epochs,percentage,first_time):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model:inital model\n",
    "        weight_list_per_epoch: weight tensors at every epoch\n",
    "        epochs:number of epochs to be trained on custom regularizer\n",
    "        percentage:percentage of filters to be pruned\n",
    "        first_time:type bool\n",
    "    Return:\n",
    "        model:optimized model\n",
    "        hisory: accuracies and losses over the process keras library\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    img_rows, img_cols = 28, 28\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    regularizer_value = my_get_regularizer_value(model,weight_list_per_epoch,percentage,first_time)\n",
    "    print(\"INITIAL REGULARIZER VALUE \",my_get_regularizer_value(model,weight_list_per_epoch,percentage,first_time))\n",
    "    model_loss = custom_loss(lmbda= 0.1 , regularizer_value=regularizer_value)\n",
    "    # print('model loss',model_loss)\n",
    "    adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss=model_loss,optimizer=adam,metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train , y_train,epochs=epochs,batch_size = batch_size,validation_data=(x_test, y_test),verbose=1)\n",
    "    print(\"FINAL REGULARIZER VALUE \",my_get_regularizer_value(model,weight_list_per_epoch,percentage,first_time))\n",
    "    return model,history\n",
    "\n",
    "count_model_params_flops(model,True)\n",
    "\n",
    "print('Validation accuracy ',max(history.history['val_accuracy']))\n",
    "\n",
    "validation_accuracy = max(history.history['val_accuracy'])\n",
    "print(\"Initial Validation Accuracy = {}\".format(validation_accuracy) )\n",
    "max_val_acc = validation_accuracy\n",
    "count = 0\n",
    "all_models = list()\n",
    "a,b = count_model_params_flops(model,False)\n",
    "print(a,b)\n",
    "\n",
    "#######################################################################\n",
    "###################  Model Pruning\n",
    "#######################################################################\n",
    "\n",
    "while validation_accuracy - max_val_acc >= -0.01:\n",
    "\n",
    "    print(\"ITERATION {} \".format(count+1))\n",
    "    all_models.append(model)\n",
    "    if max_val_acc < validation_accuracy:\n",
    "        max_val_acc = validation_accuracy\n",
    "        \n",
    "\n",
    "    if count < 1:\n",
    "        optimize(model,weight_list_per_epoch,10,50,True)\n",
    "        model = my_delete_filters(model,weight_list_per_epoch,50,True)\n",
    "        model,history,weight_list_per_epoch = train(model,10,False)\n",
    "   \n",
    "    else:\n",
    "        optimize(model,weight_list_per_epoch,10,30,False)\n",
    "        model = my_delete_filters(model,weight_list_per_epoch,30,False)\n",
    "        model,history,weight_list_per_epoch = train(model,20,False)\n",
    "\n",
    "    a,b = count_model_params_flops(model,False)\n",
    "    print(a,b)\n",
    "    \n",
    "    # al+=history\n",
    "    validation_accuracy = max(history.history['val_accuracy'])\n",
    "    best_acc_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "    log_dict['train_loss'].append(history.history['loss'][best_acc_index])\n",
    "    log_dict['train_acc'].append(history.history['accuracy'][best_acc_index])\n",
    "    log_dict['val_loss'].append(history.history['val_loss'][best_acc_index])\n",
    "    log_dict['val_acc'].append(history.history['val_accuracy'][best_acc_index])\n",
    "    a,b = count_model_params_flops(model,False)\n",
    "    log_dict['total_params'].append(a)\n",
    "    log_dict['total_flops'].append(b)\n",
    "    log_dict['filters_in_conv1'].append(model.layers[1].get_weights()[0].shape[-1])\n",
    "    log_dict['filters_in_conv2'].append(model.layers[3].get_weights()[0].shape[-1])\n",
    "    print(\"VALIDATION ACCURACY AFTER {} ITERATIONS = {}\".format(count+1,validation_accuracy))\n",
    "    count+=1\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# l1_norms = my_get_l1_norms_filters_per_epoch(weight_list_per_epoch)\n",
    "# distance_matrix_list = my_get_distance_matrix_list(l1_norms)\n",
    "# episodes_for_all_layers = my_get_episodes_for_all_layers(distance_matrix_list,95)\n",
    "# print(episodes_for_all_layers)\n",
    "# filter_pruning_indices = my_get_filter_pruning_indices(episodes_for_all_layers,l1_norms)\n",
    "# print(filter_pruning_indices[0],filter_pruning_indices[1])\n",
    "\n",
    "# optimize(model,weight_list_per_epoch,20,40,False)\n",
    "\n",
    "# surgeon = Surgeon(model)\n",
    "# surgeon.add_job('delete_channels',model.layers[1],channels = filter_pruning_indices[0][:1])\n",
    "# surgeon.add_job('delete_channels',model.layers[3],channels =filter_pruning_indices[1][:1])\n",
    "# model = surgeon.operate()\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "model,history,weight_list_per_epoch = train(model,60,False)\n",
    "\n",
    "best_acc_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "log_dict['train_loss'].append(history.history['loss'][best_acc_index])\n",
    "log_dict['train_acc'].append(history.history['accuracy'][best_acc_index])\n",
    "log_dict['val_loss'].append(history.history['val_loss'][best_acc_index])\n",
    "log_dict['val_acc'].append(history.history['val_accuracy'][best_acc_index])\n",
    "a,b = count_model_params_flops(model,False)\n",
    "log_dict['total_params'].append(a)\n",
    "log_dict['total_flops'].append(b)\n",
    "log_dict['filters_in_conv1'].append(model.layers[1].get_weights()[0].shape[-1])\n",
    "log_dict['filters_in_conv2'].append(model.layers[3].get_weights()[0].shape[-1])\n",
    "print(\"Final Validation Accuracy = \",(max(history.history['val_accuracy'])*100))\n",
    "\n",
    "log_df = pd.DataFrame(log_dict)\n",
    "log_df\n",
    "\n",
    "log_df.to_csv(os.path.join('.', 'results', 'lenet5_2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb994fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1bc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
