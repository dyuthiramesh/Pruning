{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7a477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchsummary import summary\n",
    "from thop import profile\n",
    "\n",
    "# Initialize random seed for reproducibility\n",
    "seed = 1787\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "th.cuda.manual_seed(seed)\n",
    "th.cuda.manual_seed_all(seed)\n",
    "th.backends.cudnn.deterministic = True\n",
    "th.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "epochs = 1\n",
    "custom_epochs = 1\n",
    "new_epochs = 1\n",
    "'''prune_percentage = [0.04, 0.12]\n",
    "prune_limits = [1, 2]'''\n",
    "prune_value=[1,2,4]\n",
    "prune_limits=[8]*36 + [15]*36 + [30]*36\n",
    "\n",
    "optim_lr = 0.1\n",
    "lamda = 0.01\n",
    "alpha = 0.0001\n",
    "beta = 0.0001\n",
    "\n",
    "regularization_prune_percentage = 0.02\n",
    "decorrelation_lower_bound = 0.3\n",
    "decorrelation_higher_bound = 0.4\n",
    "\n",
    "trainloader = th.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('../data', download=True, train=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "testloader = th.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('../data', download=True, train=False,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "class Network():\n",
    "\n",
    "    def weight_init(self, m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            if self.a_type == 'relu':\n",
    "                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'leaky_relu':\n",
    "                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'tanh':\n",
    "                g = init.calculate_gain(self.a_type)\n",
    "                init.xavier_uniform_(m.weight.data, gain=g)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'sigmoid':\n",
    "                g = init.calculate_gain(self.a_type)\n",
    "                init.xavier_uniform_(m.weight.data, gain=g)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            else:\n",
    "                raise\n",
    "                return NotImplemented\n",
    "\n",
    "\n",
    "    def one_hot(self, y, gpu):\n",
    "\n",
    "        try:\n",
    "            y = th.from_numpy(y)\n",
    "        except TypeError:\n",
    "            None\n",
    "\n",
    "        y_1d = y\n",
    "        if gpu:\n",
    "            y_hot = th.zeros((y.size(0), th.max(y).int()+1)).cuda()\n",
    "        else:\n",
    "            y_hot = th.zeros((y.size(0), th.max(y).int()+1))\n",
    "\n",
    "        for i in range(y.size(0)):\n",
    "            y_hot[i, y_1d[i].int()] = 1\n",
    "\n",
    "        return y_hot\n",
    "\n",
    "   \n",
    "    def best_tetr_acc(self, prunes):\n",
    "        print(\"prunes values id \", prunes)\n",
    "        tr_acc = self.train_accuracy[prunes:]\n",
    "        te_acc = self.test_accuracy[prunes:]\n",
    "        best_te_acc = max(te_acc)\n",
    "        indices = [i for i, x in enumerate(te_acc) if x == best_te_acc]\n",
    "        temp_tr_acc = []\n",
    "        for i in indices:\n",
    "            temp_tr_acc.append(tr_acc[i])\n",
    "        best_tr_acc = max(temp_tr_acc)\n",
    "\n",
    "        del self.test_accuracy[prunes:]\n",
    "        del self.train_accuracy[prunes:]\n",
    "        self.test_accuracy.append(best_te_acc)\n",
    "        self.train_accuracy.append(best_tr_acc)\n",
    "        return best_te_acc, best_tr_acc\n",
    "\n",
    "    def best_tetr_acc(self):\n",
    "        tr_acc = self.train_accuracy[:]\n",
    "        te_acc = self.test_accuracy[:]\n",
    "        best_te_acc = max(te_acc)\n",
    "        indices = [i for i, x in enumerate(te_acc) if x == best_te_acc]\n",
    "        temp_tr_acc = []\n",
    "        for i in indices:\n",
    "            temp_tr_acc.append(tr_acc[i])\n",
    "        best_tr_acc = max(temp_tr_acc)\n",
    "\n",
    "        del self.test_accuracy[prunes:]\n",
    "        del self.train_accuracy[prunes:]\n",
    "        self.test_accuracy.append(best_te_acc)\n",
    "        self.train_accuracy.append(best_tr_acc)\n",
    "        return best_te_acc, best_tr_acc\n",
    "\n",
    "    def create_folders(self, total_convs):\n",
    "        main_dir = strftime(\"/Results/%b%d_%H:%M:%S%p\", localtime()) + \"_resnet_56/\"\n",
    "        import os\n",
    "        current_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "        par_dir = os.path.abspath(current_dir + \"/../\")\n",
    "        parent_dir = par_dir + main_dir\n",
    "        path2 = os.path.join(parent_dir, \"layer_file_info\")\n",
    "        os.makedirs(path2)\n",
    "        return parent_dir\n",
    "\n",
    "    def get_writerow(self, k):\n",
    "        s = 'wr.writerow(['\n",
    "\n",
    "        for i in range(k):\n",
    "            s = s + 'd[' + str(i) + ']'\n",
    "            if i < k - 1:\n",
    "                s = s + ','\n",
    "            else:\n",
    "                s = s + '])'\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "    def get_logger(self,file_path):\n",
    "\n",
    "        logger = logging.getLogger('gal')\n",
    "        log_format = '%(asctime)s | %(message)s'\n",
    "        formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "        file_handler = logging.FileHandler(file_path)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formatter)\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(stream_handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        return logger\n",
    "\n",
    "class PruningMethod:\n",
    "    def prune_filters(self, layer_indices):\n",
    "        conv_layer = 0\n",
    "        for layer_name, layer_module in self.named_modules():\n",
    "            if isinstance(layer_module, th.nn.Conv2d):\n",
    "                if conv_layer == 0:\n",
    "                    in_channels = [i for i in range(layer_module.weight.shape[1])]\n",
    "                else:\n",
    "                    in_channels = layer_indices[conv_layer - 1]\n",
    "\n",
    "                out_channels = layer_indices[conv_layer]\n",
    "                \n",
    "                print('conv_layer:', conv_layer)\n",
    "                print('in_channels:', in_channels)\n",
    "                print('out_channels:', out_channels)\n",
    "                \n",
    "                layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])))\n",
    "                \n",
    "                print(\"1:\", layer_module.weight)\n",
    "\n",
    "                if layer_module.bias is not None:\n",
    "                    layer_module.bias = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.bias.data.cpu().numpy()[out_channels])).to('cuda'))\n",
    "\n",
    "                layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.numpy()[:, in_channels])).to('cuda'))\n",
    "                \n",
    "                print(\"2:\", layer_module.weight)\n",
    "\n",
    "                layer_module.in_channels = len(in_channels)\n",
    "                layer_module.out_channels = len(out_channels)\n",
    "                \n",
    "                conv_layer += 1\n",
    "\n",
    "            if isinstance(layer_module, th.nn.BatchNorm2d):\n",
    "                out_channels = layer_indices[conv_layer]\n",
    "                layer_module.weight = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])).to('cuda'))\n",
    "                layer_module.bias = th.nn.Parameter(th.FloatTensor(th.from_numpy(layer_module.bias.data.cpu().numpy()[out_channels])).to('cuda'))\n",
    "                layer_module.running_mean = th.from_numpy(layer_module.running_mean.cpu().numpy()[out_channels]).to('cuda')\n",
    "                layer_module.running_var = th.from_numpy(layer_module.running_var.cpu().numpy()[out_channels]).to('cuda')\n",
    "                layer_module.num_features = len(out_channels)\n",
    "\n",
    "            if isinstance(layer_module, nn.Linear):\n",
    "                conv_layer -= 1\n",
    "                in_channels = layer_indices[conv_layer]\n",
    "                weight_linear = layer_module.weight.data.cpu().numpy()\n",
    "                size = 4 * 4\n",
    "                expanded_in_channels = []\n",
    "                for i in in_channels:\n",
    "                    for j in range(size):\n",
    "                        expanded_in_channels.extend([i * size + j])\n",
    "                layer_module.weight = th.nn.Parameter(th.from_numpy(weight_linear[:, expanded_in_channels]).to('cuda'))\n",
    "                layer_module.in_features = len(expanded_in_channels)\n",
    "                break\n",
    "\n",
    "    '''def get_indices_topk(self,layer_bounds,layer_num,prune_limit,prune_value):\n",
    "\n",
    "        i=layer_num\n",
    "        indices=prune_value[i]\n",
    "\n",
    "        p=len(layer_bounds)\n",
    "        if (p-indices)<prune_limit:\n",
    "            prune_value[i]=p-prune_limit\n",
    "            indices=prune_value[i]\n",
    "\n",
    "        k=sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[:indices]\n",
    "        return k\n",
    "      \n",
    "    def get_indices_bottomk(self,layer_bounds,i,prune_limit):\n",
    "\n",
    "        k=sorted(range(len(layer_bounds)), key=lambda j: layer_bounds[j])[-prune_limit:]\n",
    "        return k'''\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class ResBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(ResBasicBlock, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inplanes != planes:\n",
    "            self.shortcut = LambdaLayer(\n",
    "                lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes // 4, planes - inplanes - (planes // 4)), \"constant\", 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_layers, covcfg, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (num_layers - 2) // 6\n",
    "        self.covcfg = covcfg\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(1, block, 16, blocks=n, stride=1)\n",
    "        self.layer2 = self._make_layer(2, block, 32, blocks=n, stride=2)\n",
    "        self.layer3 = self._make_layer(3, block, 64, blocks=n, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, a, block, planes, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet_110():\n",
    "    cov_cfg = [(3 * i + 2) for i in range(9 * 6 * 2 + 1)]\n",
    "    return ResNet(ResBasicBlock, 110, cov_cfg)\n",
    "\n",
    "# Load the model\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = resnet_110().to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = th.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30], gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "conv_layers = [module for module in model.modules() if isinstance(module, nn.Conv2d)]\n",
    "\n",
    "'''for i, layer in enumerate(conv_layers):\n",
    "    with th.no_grad():\n",
    "        filters = layer.weight.data.clone()\n",
    "        num_filters = filters.size(0)\n",
    "        #print(i, layer, num_filters)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity_matrix = []\n",
    "        for j in range(num_filters):\n",
    "            for k in range(j + 1, num_filters):\n",
    "                cosine_sim = F.cosine_similarity(filters[j].flatten(), filters[k].flatten(), dim=0)\n",
    "                similarity_matrix.append((j, k, cosine_sim.item()))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e993fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "conv1\n",
      "bn1\n",
      "relu\n",
      "layer1\n",
      "layer1.0\n",
      "layer1.0.conv1\n",
      "layer1.0.bn1\n",
      "layer1.0.relu1\n",
      "layer1.0.conv2\n",
      "layer1.0.bn2\n",
      "layer1.0.relu2\n",
      "layer1.0.shortcut\n",
      "layer1.1\n",
      "layer1.1.conv1\n",
      "layer1.1.bn1\n",
      "layer1.1.relu1\n",
      "layer1.1.conv2\n",
      "layer1.1.bn2\n",
      "layer1.1.relu2\n",
      "layer1.1.shortcut\n",
      "layer1.2\n",
      "layer1.2.conv1\n",
      "layer1.2.bn1\n",
      "layer1.2.relu1\n",
      "layer1.2.conv2\n",
      "layer1.2.bn2\n",
      "layer1.2.relu2\n",
      "layer1.2.shortcut\n",
      "layer1.3\n",
      "layer1.3.conv1\n",
      "layer1.3.bn1\n",
      "layer1.3.relu1\n",
      "layer1.3.conv2\n",
      "layer1.3.bn2\n",
      "layer1.3.relu2\n",
      "layer1.3.shortcut\n",
      "layer1.4\n",
      "layer1.4.conv1\n",
      "layer1.4.bn1\n",
      "layer1.4.relu1\n",
      "layer1.4.conv2\n",
      "layer1.4.bn2\n",
      "layer1.4.relu2\n",
      "layer1.4.shortcut\n",
      "layer1.5\n",
      "layer1.5.conv1\n",
      "layer1.5.bn1\n",
      "layer1.5.relu1\n",
      "layer1.5.conv2\n",
      "layer1.5.bn2\n",
      "layer1.5.relu2\n",
      "layer1.5.shortcut\n",
      "layer1.6\n",
      "layer1.6.conv1\n",
      "layer1.6.bn1\n",
      "layer1.6.relu1\n",
      "layer1.6.conv2\n",
      "layer1.6.bn2\n",
      "layer1.6.relu2\n",
      "layer1.6.shortcut\n",
      "layer1.7\n",
      "layer1.7.conv1\n",
      "layer1.7.bn1\n",
      "layer1.7.relu1\n",
      "layer1.7.conv2\n",
      "layer1.7.bn2\n",
      "layer1.7.relu2\n",
      "layer1.7.shortcut\n",
      "layer1.8\n",
      "layer1.8.conv1\n",
      "layer1.8.bn1\n",
      "layer1.8.relu1\n",
      "layer1.8.conv2\n",
      "layer1.8.bn2\n",
      "layer1.8.relu2\n",
      "layer1.8.shortcut\n",
      "layer1.9\n",
      "layer1.9.conv1\n",
      "layer1.9.bn1\n",
      "layer1.9.relu1\n",
      "layer1.9.conv2\n",
      "layer1.9.bn2\n",
      "layer1.9.relu2\n",
      "layer1.9.shortcut\n",
      "layer1.10\n",
      "layer1.10.conv1\n",
      "layer1.10.bn1\n",
      "layer1.10.relu1\n",
      "layer1.10.conv2\n",
      "layer1.10.bn2\n",
      "layer1.10.relu2\n",
      "layer1.10.shortcut\n",
      "layer1.11\n",
      "layer1.11.conv1\n",
      "layer1.11.bn1\n",
      "layer1.11.relu1\n",
      "layer1.11.conv2\n",
      "layer1.11.bn2\n",
      "layer1.11.relu2\n",
      "layer1.11.shortcut\n",
      "layer1.12\n",
      "layer1.12.conv1\n",
      "layer1.12.bn1\n",
      "layer1.12.relu1\n",
      "layer1.12.conv2\n",
      "layer1.12.bn2\n",
      "layer1.12.relu2\n",
      "layer1.12.shortcut\n",
      "layer1.13\n",
      "layer1.13.conv1\n",
      "layer1.13.bn1\n",
      "layer1.13.relu1\n",
      "layer1.13.conv2\n",
      "layer1.13.bn2\n",
      "layer1.13.relu2\n",
      "layer1.13.shortcut\n",
      "layer1.14\n",
      "layer1.14.conv1\n",
      "layer1.14.bn1\n",
      "layer1.14.relu1\n",
      "layer1.14.conv2\n",
      "layer1.14.bn2\n",
      "layer1.14.relu2\n",
      "layer1.14.shortcut\n",
      "layer1.15\n",
      "layer1.15.conv1\n",
      "layer1.15.bn1\n",
      "layer1.15.relu1\n",
      "layer1.15.conv2\n",
      "layer1.15.bn2\n",
      "layer1.15.relu2\n",
      "layer1.15.shortcut\n",
      "layer1.16\n",
      "layer1.16.conv1\n",
      "layer1.16.bn1\n",
      "layer1.16.relu1\n",
      "layer1.16.conv2\n",
      "layer1.16.bn2\n",
      "layer1.16.relu2\n",
      "layer1.16.shortcut\n",
      "layer1.17\n",
      "layer1.17.conv1\n",
      "layer1.17.bn1\n",
      "layer1.17.relu1\n",
      "layer1.17.conv2\n",
      "layer1.17.bn2\n",
      "layer1.17.relu2\n",
      "layer1.17.shortcut\n",
      "layer2\n",
      "layer2.0\n",
      "layer2.0.conv1\n",
      "layer2.0.bn1\n",
      "layer2.0.relu1\n",
      "layer2.0.conv2\n",
      "layer2.0.bn2\n",
      "layer2.0.relu2\n",
      "layer2.0.shortcut\n",
      "layer2.1\n",
      "layer2.1.conv1\n",
      "layer2.1.bn1\n",
      "layer2.1.relu1\n",
      "layer2.1.conv2\n",
      "layer2.1.bn2\n",
      "layer2.1.relu2\n",
      "layer2.1.shortcut\n",
      "layer2.2\n",
      "layer2.2.conv1\n",
      "layer2.2.bn1\n",
      "layer2.2.relu1\n",
      "layer2.2.conv2\n",
      "layer2.2.bn2\n",
      "layer2.2.relu2\n",
      "layer2.2.shortcut\n",
      "layer2.3\n",
      "layer2.3.conv1\n",
      "layer2.3.bn1\n",
      "layer2.3.relu1\n",
      "layer2.3.conv2\n",
      "layer2.3.bn2\n",
      "layer2.3.relu2\n",
      "layer2.3.shortcut\n",
      "layer2.4\n",
      "layer2.4.conv1\n",
      "layer2.4.bn1\n",
      "layer2.4.relu1\n",
      "layer2.4.conv2\n",
      "layer2.4.bn2\n",
      "layer2.4.relu2\n",
      "layer2.4.shortcut\n",
      "layer2.5\n",
      "layer2.5.conv1\n",
      "layer2.5.bn1\n",
      "layer2.5.relu1\n",
      "layer2.5.conv2\n",
      "layer2.5.bn2\n",
      "layer2.5.relu2\n",
      "layer2.5.shortcut\n",
      "layer2.6\n",
      "layer2.6.conv1\n",
      "layer2.6.bn1\n",
      "layer2.6.relu1\n",
      "layer2.6.conv2\n",
      "layer2.6.bn2\n",
      "layer2.6.relu2\n",
      "layer2.6.shortcut\n",
      "layer2.7\n",
      "layer2.7.conv1\n",
      "layer2.7.bn1\n",
      "layer2.7.relu1\n",
      "layer2.7.conv2\n",
      "layer2.7.bn2\n",
      "layer2.7.relu2\n",
      "layer2.7.shortcut\n",
      "layer2.8\n",
      "layer2.8.conv1\n",
      "layer2.8.bn1\n",
      "layer2.8.relu1\n",
      "layer2.8.conv2\n",
      "layer2.8.bn2\n",
      "layer2.8.relu2\n",
      "layer2.8.shortcut\n",
      "layer2.9\n",
      "layer2.9.conv1\n",
      "layer2.9.bn1\n",
      "layer2.9.relu1\n",
      "layer2.9.conv2\n",
      "layer2.9.bn2\n",
      "layer2.9.relu2\n",
      "layer2.9.shortcut\n",
      "layer2.10\n",
      "layer2.10.conv1\n",
      "layer2.10.bn1\n",
      "layer2.10.relu1\n",
      "layer2.10.conv2\n",
      "layer2.10.bn2\n",
      "layer2.10.relu2\n",
      "layer2.10.shortcut\n",
      "layer2.11\n",
      "layer2.11.conv1\n",
      "layer2.11.bn1\n",
      "layer2.11.relu1\n",
      "layer2.11.conv2\n",
      "layer2.11.bn2\n",
      "layer2.11.relu2\n",
      "layer2.11.shortcut\n",
      "layer2.12\n",
      "layer2.12.conv1\n",
      "layer2.12.bn1\n",
      "layer2.12.relu1\n",
      "layer2.12.conv2\n",
      "layer2.12.bn2\n",
      "layer2.12.relu2\n",
      "layer2.12.shortcut\n",
      "layer2.13\n",
      "layer2.13.conv1\n",
      "layer2.13.bn1\n",
      "layer2.13.relu1\n",
      "layer2.13.conv2\n",
      "layer2.13.bn2\n",
      "layer2.13.relu2\n",
      "layer2.13.shortcut\n",
      "layer2.14\n",
      "layer2.14.conv1\n",
      "layer2.14.bn1\n",
      "layer2.14.relu1\n",
      "layer2.14.conv2\n",
      "layer2.14.bn2\n",
      "layer2.14.relu2\n",
      "layer2.14.shortcut\n",
      "layer2.15\n",
      "layer2.15.conv1\n",
      "layer2.15.bn1\n",
      "layer2.15.relu1\n",
      "layer2.15.conv2\n",
      "layer2.15.bn2\n",
      "layer2.15.relu2\n",
      "layer2.15.shortcut\n",
      "layer2.16\n",
      "layer2.16.conv1\n",
      "layer2.16.bn1\n",
      "layer2.16.relu1\n",
      "layer2.16.conv2\n",
      "layer2.16.bn2\n",
      "layer2.16.relu2\n",
      "layer2.16.shortcut\n",
      "layer2.17\n",
      "layer2.17.conv1\n",
      "layer2.17.bn1\n",
      "layer2.17.relu1\n",
      "layer2.17.conv2\n",
      "layer2.17.bn2\n",
      "layer2.17.relu2\n",
      "layer2.17.shortcut\n",
      "layer3\n",
      "layer3.0\n",
      "layer3.0.conv1\n",
      "layer3.0.bn1\n",
      "layer3.0.relu1\n",
      "layer3.0.conv2\n",
      "layer3.0.bn2\n",
      "layer3.0.relu2\n",
      "layer3.0.shortcut\n",
      "layer3.1\n",
      "layer3.1.conv1\n",
      "layer3.1.bn1\n",
      "layer3.1.relu1\n",
      "layer3.1.conv2\n",
      "layer3.1.bn2\n",
      "layer3.1.relu2\n",
      "layer3.1.shortcut\n",
      "layer3.2\n",
      "layer3.2.conv1\n",
      "layer3.2.bn1\n",
      "layer3.2.relu1\n",
      "layer3.2.conv2\n",
      "layer3.2.bn2\n",
      "layer3.2.relu2\n",
      "layer3.2.shortcut\n",
      "layer3.3\n",
      "layer3.3.conv1\n",
      "layer3.3.bn1\n",
      "layer3.3.relu1\n",
      "layer3.3.conv2\n",
      "layer3.3.bn2\n",
      "layer3.3.relu2\n",
      "layer3.3.shortcut\n",
      "layer3.4\n",
      "layer3.4.conv1\n",
      "layer3.4.bn1\n",
      "layer3.4.relu1\n",
      "layer3.4.conv2\n",
      "layer3.4.bn2\n",
      "layer3.4.relu2\n",
      "layer3.4.shortcut\n",
      "layer3.5\n",
      "layer3.5.conv1\n",
      "layer3.5.bn1\n",
      "layer3.5.relu1\n",
      "layer3.5.conv2\n",
      "layer3.5.bn2\n",
      "layer3.5.relu2\n",
      "layer3.5.shortcut\n",
      "layer3.6\n",
      "layer3.6.conv1\n",
      "layer3.6.bn1\n",
      "layer3.6.relu1\n",
      "layer3.6.conv2\n",
      "layer3.6.bn2\n",
      "layer3.6.relu2\n",
      "layer3.6.shortcut\n",
      "layer3.7\n",
      "layer3.7.conv1\n",
      "layer3.7.bn1\n",
      "layer3.7.relu1\n",
      "layer3.7.conv2\n",
      "layer3.7.bn2\n",
      "layer3.7.relu2\n",
      "layer3.7.shortcut\n",
      "layer3.8\n",
      "layer3.8.conv1\n",
      "layer3.8.bn1\n",
      "layer3.8.relu1\n",
      "layer3.8.conv2\n",
      "layer3.8.bn2\n",
      "layer3.8.relu2\n",
      "layer3.8.shortcut\n",
      "layer3.9\n",
      "layer3.9.conv1\n",
      "layer3.9.bn1\n",
      "layer3.9.relu1\n",
      "layer3.9.conv2\n",
      "layer3.9.bn2\n",
      "layer3.9.relu2\n",
      "layer3.9.shortcut\n",
      "layer3.10\n",
      "layer3.10.conv1\n",
      "layer3.10.bn1\n",
      "layer3.10.relu1\n",
      "layer3.10.conv2\n",
      "layer3.10.bn2\n",
      "layer3.10.relu2\n",
      "layer3.10.shortcut\n",
      "layer3.11\n",
      "layer3.11.conv1\n",
      "layer3.11.bn1\n",
      "layer3.11.relu1\n",
      "layer3.11.conv2\n",
      "layer3.11.bn2\n",
      "layer3.11.relu2\n",
      "layer3.11.shortcut\n",
      "layer3.12\n",
      "layer3.12.conv1\n",
      "layer3.12.bn1\n",
      "layer3.12.relu1\n",
      "layer3.12.conv2\n",
      "layer3.12.bn2\n",
      "layer3.12.relu2\n",
      "layer3.12.shortcut\n",
      "layer3.13\n",
      "layer3.13.conv1\n",
      "layer3.13.bn1\n",
      "layer3.13.relu1\n",
      "layer3.13.conv2\n",
      "layer3.13.bn2\n",
      "layer3.13.relu2\n",
      "layer3.13.shortcut\n",
      "layer3.14\n",
      "layer3.14.conv1\n",
      "layer3.14.bn1\n",
      "layer3.14.relu1\n",
      "layer3.14.conv2\n",
      "layer3.14.bn2\n",
      "layer3.14.relu2\n",
      "layer3.14.shortcut\n",
      "layer3.15\n",
      "layer3.15.conv1\n",
      "layer3.15.bn1\n",
      "layer3.15.relu1\n",
      "layer3.15.conv2\n",
      "layer3.15.bn2\n",
      "layer3.15.relu2\n",
      "layer3.15.shortcut\n",
      "layer3.16\n",
      "layer3.16.conv1\n",
      "layer3.16.bn1\n",
      "layer3.16.relu1\n",
      "layer3.16.conv2\n",
      "layer3.16.bn2\n",
      "layer3.16.relu2\n",
      "layer3.16.shortcut\n",
      "layer3.17\n",
      "layer3.17.conv1\n",
      "layer3.17.bn1\n",
      "layer3.17.relu1\n",
      "layer3.17.conv2\n",
      "layer3.17.bn2\n",
      "layer3.17.relu2\n",
      "layer3.17.shortcut\n",
      "avgpool\n",
      "fc\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38cb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
