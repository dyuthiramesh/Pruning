{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23d0e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torchsummary import summary\n",
    "from thop import profile\n",
    "\n",
    "# Initialize random seed for reproducibility\n",
    "seed = 1787\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "th.manual_seed(seed)\n",
    "th.cuda.manual_seed(seed)\n",
    "th.cuda.manual_seed_all(seed)\n",
    "th.backends.cudnn.deterministic = True\n",
    "th.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set device\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "custom_epochs = 1\n",
    "AP_custom_epochs = 1\n",
    "\n",
    "prune_value=[1,2,4]\n",
    "#prune_limits=[6]*18*3\n",
    "prune_limits=[8]*18 + [15]*18 + [30]*18 #Make it 9, 9, 9, figure out what these 2 actually mean\n",
    "#8, 17, 34s\n",
    "\n",
    "optim_lr = 0.1\n",
    "lamda = 0.01\n",
    "alpha = 0.0001\n",
    "beta = 0.0001\n",
    "\n",
    "AP_alpha = 0.0001\n",
    "\n",
    "regularization_prune_percentage = 0.02\n",
    "decorrelation_lower_bound = 0.3\n",
    "decorrelation_higher_bound = 0.4\n",
    "\n",
    "trainloader = th.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('../data', download=True, train=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "testloader = th.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10('../data', download=True, train=False,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "class Network():\n",
    "\n",
    "    def weight_init(self, m):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            if self.a_type == 'relu':\n",
    "                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'leaky_relu':\n",
    "                init.kaiming_normal_(m.weight.data, nonlinearity=self.a_type)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'tanh':\n",
    "                g = init.calculate_gain(self.a_type)\n",
    "                init.xavier_uniform_(m.weight.data, gain=g)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            elif self.a_type == 'sigmoid':\n",
    "                g = init.calculate_gain(self.a_type)\n",
    "                init.xavier_uniform_(m.weight.data, gain=g)\n",
    "                init.constant_(m.bias.data, 0)\n",
    "            else:\n",
    "                raise\n",
    "                return NotImplemented\n",
    "\n",
    "\n",
    "    def one_hot(self, y, gpu):\n",
    "\n",
    "        try:\n",
    "            y = th.from_numpy(y)\n",
    "        except TypeError:\n",
    "            None\n",
    "\n",
    "        y_1d = y\n",
    "        if gpu:\n",
    "            y_hot = th.zeros((y.size(0), th.max(y).int()+1)).cuda()\n",
    "        else:\n",
    "            y_hot = th.zeros((y.size(0), th.max(y).int()+1))\n",
    "\n",
    "        for i in range(y.size(0)):\n",
    "            y_hot[i, y_1d[i].int()] = 1\n",
    "\n",
    "        return y_hot\n",
    "\n",
    "   \n",
    "    def best_tetr_acc(self, prunes):\n",
    "        print(\"prunes values id \", prunes)\n",
    "        tr_acc = self.train_accuracy[prunes:]\n",
    "        te_acc = self.test_accuracy[prunes:]\n",
    "        best_te_acc = max(te_acc)\n",
    "        indices = [i for i, x in enumerate(te_acc) if x == best_te_acc]\n",
    "        temp_tr_acc = []\n",
    "        for i in indices:\n",
    "            temp_tr_acc.append(tr_acc[i])\n",
    "        best_tr_acc = max(temp_tr_acc)\n",
    "\n",
    "        del self.test_accuracy[prunes:]\n",
    "        del self.train_accuracy[prunes:]\n",
    "        self.test_accuracy.append(best_te_acc)\n",
    "        self.train_accuracy.append(best_tr_acc)\n",
    "        return best_te_acc, best_tr_acc\n",
    "\n",
    "    def best_tetr_acc(self):\n",
    "        tr_acc = self.train_accuracy[:]\n",
    "        te_acc = self.test_accuracy[:]\n",
    "        best_te_acc = max(te_acc)\n",
    "        indices = [i for i, x in enumerate(te_acc) if x == best_te_acc]\n",
    "        temp_tr_acc = []\n",
    "        for i in indices:\n",
    "            temp_tr_acc.append(tr_acc[i])\n",
    "        best_tr_acc = max(temp_tr_acc)\n",
    "\n",
    "        del self.test_accuracy[prunes:]\n",
    "        del self.train_accuracy[prunes:]\n",
    "        self.test_accuracy.append(best_te_acc)\n",
    "        self.train_accuracy.append(best_tr_acc)\n",
    "        return best_te_acc, best_tr_acc\n",
    "\n",
    "    def create_folders(self, total_convs):\n",
    "        main_dir = strftime(\"/Results/%b%d_%H:%M:%S%p\", localtime()) + \"_resnet_110/\"\n",
    "        import os\n",
    "        current_dir = os.path.abspath(os.path.dirname(__file__))\n",
    "        par_dir = os.path.abspath(current_dir + \"/../\")\n",
    "        parent_dir = par_dir + main_dir\n",
    "        path2 = os.path.join(parent_dir, \"layer_file_info\")\n",
    "        os.makedirs(path2)\n",
    "        return parent_dir\n",
    "\n",
    "    def get_writerow(self, k):\n",
    "        s = 'wr.writerow(['\n",
    "\n",
    "        for i in range(k):\n",
    "            s = s + 'd[' + str(i) + ']'\n",
    "            if i < k - 1:\n",
    "                s = s + ','\n",
    "            else:\n",
    "                s = s + '])'\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "    def get_logger(self,file_path):\n",
    "\n",
    "        logger = logging.getLogger('gal')\n",
    "        log_format = '%(asctime)s | %(message)s'\n",
    "        formatter = logging.Formatter(log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "        file_handler = logging.FileHandler(file_path)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formatter)\n",
    "\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(stream_handler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        return logger\n",
    "\n",
    "class PruningMethod:\n",
    "    \n",
    "    def prune_filters(self, indices):\n",
    "        conv_layer = 0\n",
    "\n",
    "        for layer_name, layer_module in self.named_modules():\n",
    "            if isinstance(layer_module, th.nn.Conv2d) and layer_name != 'conv1':\n",
    "                if layer_name.find('conv1') != -1:\n",
    "                    in_channels = [i for i in range(layer_module.weight.shape[1])]\n",
    "                    out_channels = indices[conv_layer]\n",
    "                    layer_module.weight = th.nn.Parameter(\n",
    "                        th.FloatTensor(\n",
    "                            th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])\n",
    "                        ).to('cuda')\n",
    "                    )\n",
    "\n",
    "                if layer_name.find('conv2') != -1:\n",
    "                    in_channels = indices[conv_layer]\n",
    "                    out_channels = [i for i in range(layer_module.weight.shape[0])]\n",
    "                    layer_module.weight = th.nn.Parameter(\n",
    "                        th.FloatTensor(\n",
    "                            th.from_numpy(layer_module.weight.data.cpu().numpy()[:, in_channels])\n",
    "                        ).to('cuda')\n",
    "                    )\n",
    "                    conv_layer += 1\n",
    "\n",
    "                layer_module.in_channels = len(in_channels)\n",
    "                layer_module.out_channels = len(out_channels)\n",
    "\n",
    "            if isinstance(layer_module, th.nn.BatchNorm2d) and layer_name != 'bn1' and layer_name.find('bn1') != -1:\n",
    "                out_channels = indices[conv_layer]\n",
    "\n",
    "                layer_module.weight = th.nn.Parameter(\n",
    "                    th.FloatTensor(\n",
    "                        th.from_numpy(layer_module.weight.data.cpu().numpy()[out_channels])\n",
    "                    ).to('cuda')\n",
    "                )\n",
    "                layer_module.bias = th.nn.Parameter(\n",
    "                    th.FloatTensor(\n",
    "                        th.from_numpy(layer_module.bias.data.cpu().numpy()[out_channels])\n",
    "                    ).to('cuda')\n",
    "                )\n",
    "                \n",
    "                layer_module.running_mean = th.from_numpy(\n",
    "                    layer_module.running_mean.cpu().numpy()[out_channels]\n",
    "                ).to('cuda')\n",
    "                layer_module.running_var = th.from_numpy(\n",
    "                    layer_module.running_var.cpu().numpy()[out_channels]\n",
    "                ).to('cuda')\n",
    "\n",
    "                layer_module.num_features = len(out_channels)\n",
    "\n",
    "            if isinstance(layer_module, nn.Linear):\n",
    "                break\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "class ResBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(ResBasicBlock, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        self.planes = planes\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.stride = stride\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or inplanes != planes:\n",
    "            self.shortcut = LambdaLayer(\n",
    "                lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes // 4, planes - inplanes - (planes // 4)), \"constant\", 0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_layers, covcfg, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert (num_layers - 2) % 6 == 0, 'depth should be 6n+2'\n",
    "        n = (num_layers - 2) // 6\n",
    "        self.covcfg = covcfg\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(1, block, 16, blocks=n, stride=1)\n",
    "        self.layer2 = self._make_layer(2, block, 32, blocks=n, stride=2)\n",
    "        self.layer3 = self._make_layer(3, block, 64, blocks=n, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.linear = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, a, block, planes, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "def resnet_110():\n",
    "    cov_cfg = [(3 * i + 2) for i in range(9 * 6 * 2 + 1)]\n",
    "    return ResNet(ResBasicBlock, 110, cov_cfg)\n",
    "\n",
    "# Load the model\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = resnet_110().to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = th.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = th.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30], gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56f7cf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "checkpoint = th.load('base.pth')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1bbe8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 97.63%\n",
      "Test Accuracy: 90.30%\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "with th.no_grad():\n",
    "    \n",
    "    for inputs, targets in trainloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = model(inputs)\n",
    "        y_hat = th.argmax(output, 1)\n",
    "        train_acc.append((y_hat == targets).sum().item())\n",
    "    \n",
    "    for inputs, targets in testloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = model(inputs)\n",
    "        y_hat = th.argmax(output, 1)\n",
    "        test_acc.append((y_hat == targets).sum().item())\n",
    "\n",
    "train_acc = sum(train_acc) * 100 / len(trainloader.dataset)\n",
    "test_acc = sum(test_acc) * 100 / len(testloader.dataset)\n",
    "\n",
    "print(f'Train Accuracy: {train_acc:.2f}%')\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ecec15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
