/workspace/Pruning/ResNet56/56main.py:358: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = th.load('resnet56_base.pth')
/workspace/Pruning/ResNet56/56main.py:534: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = th.load('test_model.pth')
/workspace/Pruning/ResNet56/56main.py:579: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = th.load('test_model.pth')
Files already downloaded and verified
Files already downloaded and verified
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
Selected indices list: [[14], [2], [15], [7], [9], [7], [5], [15], [11], [21, 15], [17, 5], [9, 12], [4, 14], [11, 30], [3, 22], [25, 19], [21, 7], [8, 19], [33, 13, 30, 15], [32, 17, 36, 8], [9, 2, 11, 1], [16, 24, 11, 22], [3, 51, 59, 21], [1, 42, 2, 61], [34, 3, 63, 23], [16, 21, 37, 46], [11, 58, 35, 42]]
Remaining indices list: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15], [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15], [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15], [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15], [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31], [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], [0, 3, 4, 5, 6, 7, 8, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 56, 57, 58, 60, 61, 62, 63], [0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63], [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62, 63]]
Epoch [1/1], Train Accuracy: 94.21%
Pruning Iteration 1, Epoch [1/1], Old Loss: 0.17053246, New Loss: 0.16777045
Regularization term: 1.7945995330810547, Decorrelation term: 5.806279182434082
Epoch [1/1], Test Accuracy: 87.34%
Stage 1 - Remaining Filters: 16
Stage 2 - Remaining Filters: 32
Stage 3 - Remaining Filters: 64
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
Total FLOPs: 127619776.0, Total Params: 853018.0
Starting AP Training
AP_Epoch [1/1], AP Train Accuracy: 93.32%,  AP Old Loss: 0.19353426, AP New Loss: 0.23032913, AP Finetuning term: 0.0035082732792943716
AP Epoch [1/1], Test Accuracy: 87.01%
2.841446 0.0 0.0
2875.953771 47.0 0.0
0.015795 0.0 0.0
29.4347 0.0 0.0
Iteration Completed


Experiment completed successfully.
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 32, 32]             432
       BatchNorm2d-2           [-1, 16, 32, 32]              32
              ReLU-3           [-1, 16, 32, 32]               0
            Conv2d-4           [-1, 15, 32, 32]           2,160
       BatchNorm2d-5           [-1, 15, 32, 32]              30
              ReLU-6           [-1, 15, 32, 32]               0
            Conv2d-7           [-1, 16, 32, 32]           2,160
       BatchNorm2d-8           [-1, 16, 32, 32]              32
              ReLU-9           [-1, 16, 32, 32]               0
    ResBasicBlock-10           [-1, 16, 32, 32]               0
           Conv2d-11           [-1, 15, 32, 32]           2,160
      BatchNorm2d-12           [-1, 15, 32, 32]              30
             ReLU-13           [-1, 15, 32, 32]               0
           Conv2d-14           [-1, 16, 32, 32]           2,160
      BatchNorm2d-15           [-1, 16, 32, 32]              32
             ReLU-16           [-1, 16, 32, 32]               0
    ResBasicBlock-17           [-1, 16, 32, 32]               0
           Conv2d-18           [-1, 15, 32, 32]           2,160
      BatchNorm2d-19           [-1, 15, 32, 32]              30
             ReLU-20           [-1, 15, 32, 32]               0
           Conv2d-21           [-1, 16, 32, 32]           2,160
      BatchNorm2d-22           [-1, 16, 32, 32]              32
             ReLU-23           [-1, 16, 32, 32]               0
    ResBasicBlock-24           [-1, 16, 32, 32]               0
           Conv2d-25           [-1, 15, 32, 32]           2,160
      BatchNorm2d-26           [-1, 15, 32, 32]              30
             ReLU-27           [-1, 15, 32, 32]               0
           Conv2d-28           [-1, 16, 32, 32]           2,160
      BatchNorm2d-29           [-1, 16, 32, 32]              32
             ReLU-30           [-1, 16, 32, 32]               0
    ResBasicBlock-31           [-1, 16, 32, 32]               0
           Conv2d-32           [-1, 15, 32, 32]           2,160
      BatchNorm2d-33           [-1, 15, 32, 32]              30
             ReLU-34           [-1, 15, 32, 32]               0
           Conv2d-35           [-1, 16, 32, 32]           2,160
      BatchNorm2d-36           [-1, 16, 32, 32]              32
             ReLU-37           [-1, 16, 32, 32]               0
    ResBasicBlock-38           [-1, 16, 32, 32]               0
           Conv2d-39           [-1, 15, 32, 32]           2,160
      BatchNorm2d-40           [-1, 15, 32, 32]              30
             ReLU-41           [-1, 15, 32, 32]               0
           Conv2d-42           [-1, 16, 32, 32]           2,160
      BatchNorm2d-43           [-1, 16, 32, 32]              32
             ReLU-44           [-1, 16, 32, 32]               0
    ResBasicBlock-45           [-1, 16, 32, 32]               0
           Conv2d-46           [-1, 15, 32, 32]           2,160
      BatchNorm2d-47           [-1, 15, 32, 32]              30
             ReLU-48           [-1, 15, 32, 32]               0
           Conv2d-49           [-1, 16, 32, 32]           2,160
      BatchNorm2d-50           [-1, 16, 32, 32]              32
             ReLU-51           [-1, 16, 32, 32]               0
    ResBasicBlock-52           [-1, 16, 32, 32]               0
           Conv2d-53           [-1, 15, 32, 32]           2,160
      BatchNorm2d-54           [-1, 15, 32, 32]              30
             ReLU-55           [-1, 15, 32, 32]               0
           Conv2d-56           [-1, 16, 32, 32]           2,160
      BatchNorm2d-57           [-1, 16, 32, 32]              32
             ReLU-58           [-1, 16, 32, 32]               0
    ResBasicBlock-59           [-1, 16, 32, 32]               0
           Conv2d-60           [-1, 15, 32, 32]           2,160
      BatchNorm2d-61           [-1, 15, 32, 32]              30
             ReLU-62           [-1, 15, 32, 32]               0
           Conv2d-63           [-1, 16, 32, 32]           2,160
      BatchNorm2d-64           [-1, 16, 32, 32]              32
             ReLU-65           [-1, 16, 32, 32]               0
    ResBasicBlock-66           [-1, 16, 32, 32]               0
           Conv2d-67           [-1, 30, 16, 16]           4,320
      BatchNorm2d-68           [-1, 30, 16, 16]              60
             ReLU-69           [-1, 30, 16, 16]               0
           Conv2d-70           [-1, 32, 16, 16]           8,640
      BatchNorm2d-71           [-1, 32, 16, 16]              64
      LambdaLayer-72           [-1, 32, 16, 16]               0
             ReLU-73           [-1, 32, 16, 16]               0
    ResBasicBlock-74           [-1, 32, 16, 16]               0
           Conv2d-75           [-1, 30, 16, 16]           8,640
      BatchNorm2d-76           [-1, 30, 16, 16]              60
             ReLU-77           [-1, 30, 16, 16]               0
           Conv2d-78           [-1, 32, 16, 16]           8,640
      BatchNorm2d-79           [-1, 32, 16, 16]              64
             ReLU-80           [-1, 32, 16, 16]               0
    ResBasicBlock-81           [-1, 32, 16, 16]               0
           Conv2d-82           [-1, 30, 16, 16]           8,640
      BatchNorm2d-83           [-1, 30, 16, 16]              60
             ReLU-84           [-1, 30, 16, 16]               0
           Conv2d-85           [-1, 32, 16, 16]           8,640
      BatchNorm2d-86           [-1, 32, 16, 16]              64
             ReLU-87           [-1, 32, 16, 16]               0
    ResBasicBlock-88           [-1, 32, 16, 16]               0
           Conv2d-89           [-1, 30, 16, 16]           8,640
      BatchNorm2d-90           [-1, 30, 16, 16]              60
             ReLU-91           [-1, 30, 16, 16]               0
           Conv2d-92           [-1, 32, 16, 16]           8,640
      BatchNorm2d-93           [-1, 32, 16, 16]              64
             ReLU-94           [-1, 32, 16, 16]               0
    ResBasicBlock-95           [-1, 32, 16, 16]               0
           Conv2d-96           [-1, 30, 16, 16]           8,640
      BatchNorm2d-97           [-1, 30, 16, 16]              60
             ReLU-98           [-1, 30, 16, 16]               0
           Conv2d-99           [-1, 32, 16, 16]           8,640
     BatchNorm2d-100           [-1, 32, 16, 16]              64
            ReLU-101           [-1, 32, 16, 16]               0
   ResBasicBlock-102           [-1, 32, 16, 16]               0
          Conv2d-103           [-1, 30, 16, 16]           8,640
     BatchNorm2d-104           [-1, 30, 16, 16]              60
            ReLU-105           [-1, 30, 16, 16]               0
          Conv2d-106           [-1, 32, 16, 16]           8,640
     BatchNorm2d-107           [-1, 32, 16, 16]              64
            ReLU-108           [-1, 32, 16, 16]               0
   ResBasicBlock-109           [-1, 32, 16, 16]               0
          Conv2d-110           [-1, 30, 16, 16]           8,640
     BatchNorm2d-111           [-1, 30, 16, 16]              60
            ReLU-112           [-1, 30, 16, 16]               0
          Conv2d-113           [-1, 32, 16, 16]           8,640
     BatchNorm2d-114           [-1, 32, 16, 16]              64
            ReLU-115           [-1, 32, 16, 16]               0
   ResBasicBlock-116           [-1, 32, 16, 16]               0
          Conv2d-117           [-1, 30, 16, 16]           8,640
     BatchNorm2d-118           [-1, 30, 16, 16]              60
            ReLU-119           [-1, 30, 16, 16]               0
          Conv2d-120           [-1, 32, 16, 16]           8,640
     BatchNorm2d-121           [-1, 32, 16, 16]              64
            ReLU-122           [-1, 32, 16, 16]               0
   ResBasicBlock-123           [-1, 32, 16, 16]               0
          Conv2d-124           [-1, 30, 16, 16]           8,640
     BatchNorm2d-125           [-1, 30, 16, 16]              60
            ReLU-126           [-1, 30, 16, 16]               0
          Conv2d-127           [-1, 32, 16, 16]           8,640
     BatchNorm2d-128           [-1, 32, 16, 16]              64
            ReLU-129           [-1, 32, 16, 16]               0
   ResBasicBlock-130           [-1, 32, 16, 16]               0
          Conv2d-131             [-1, 60, 8, 8]          17,280
     BatchNorm2d-132             [-1, 60, 8, 8]             120
            ReLU-133             [-1, 60, 8, 8]               0
          Conv2d-134             [-1, 64, 8, 8]          34,560
     BatchNorm2d-135             [-1, 64, 8, 8]             128
     LambdaLayer-136             [-1, 64, 8, 8]               0
            ReLU-137             [-1, 64, 8, 8]               0
   ResBasicBlock-138             [-1, 64, 8, 8]               0
          Conv2d-139             [-1, 60, 8, 8]          34,560
     BatchNorm2d-140             [-1, 60, 8, 8]             120
            ReLU-141             [-1, 60, 8, 8]               0
          Conv2d-142             [-1, 64, 8, 8]          34,560
     BatchNorm2d-143             [-1, 64, 8, 8]             128
            ReLU-144             [-1, 64, 8, 8]               0
   ResBasicBlock-145             [-1, 64, 8, 8]               0
          Conv2d-146             [-1, 60, 8, 8]          34,560
     BatchNorm2d-147             [-1, 60, 8, 8]             120
            ReLU-148             [-1, 60, 8, 8]               0
          Conv2d-149             [-1, 64, 8, 8]          34,560
     BatchNorm2d-150             [-1, 64, 8, 8]             128
            ReLU-151             [-1, 64, 8, 8]               0
   ResBasicBlock-152             [-1, 64, 8, 8]               0
          Conv2d-153             [-1, 60, 8, 8]          34,560
     BatchNorm2d-154             [-1, 60, 8, 8]             120
            ReLU-155             [-1, 60, 8, 8]               0
          Conv2d-156             [-1, 64, 8, 8]          34,560
     BatchNorm2d-157             [-1, 64, 8, 8]             128
            ReLU-158             [-1, 64, 8, 8]               0
   ResBasicBlock-159             [-1, 64, 8, 8]               0
          Conv2d-160             [-1, 60, 8, 8]          34,560
     BatchNorm2d-161             [-1, 60, 8, 8]             120
            ReLU-162             [-1, 60, 8, 8]               0
          Conv2d-163             [-1, 64, 8, 8]          34,560
     BatchNorm2d-164             [-1, 64, 8, 8]             128
            ReLU-165             [-1, 64, 8, 8]               0
   ResBasicBlock-166             [-1, 64, 8, 8]               0
          Conv2d-167             [-1, 60, 8, 8]          34,560
     BatchNorm2d-168             [-1, 60, 8, 8]             120
            ReLU-169             [-1, 60, 8, 8]               0
          Conv2d-170             [-1, 64, 8, 8]          34,560
     BatchNorm2d-171             [-1, 64, 8, 8]             128
            ReLU-172             [-1, 64, 8, 8]               0
   ResBasicBlock-173             [-1, 64, 8, 8]               0
          Conv2d-174             [-1, 60, 8, 8]          34,560
     BatchNorm2d-175             [-1, 60, 8, 8]             120
            ReLU-176             [-1, 60, 8, 8]               0
          Conv2d-177             [-1, 64, 8, 8]          34,560
     BatchNorm2d-178             [-1, 64, 8, 8]             128
            ReLU-179             [-1, 64, 8, 8]               0
   ResBasicBlock-180             [-1, 64, 8, 8]               0
          Conv2d-181             [-1, 60, 8, 8]          34,560
     BatchNorm2d-182             [-1, 60, 8, 8]             120
            ReLU-183             [-1, 60, 8, 8]               0
          Conv2d-184             [-1, 64, 8, 8]          34,560
     BatchNorm2d-185             [-1, 64, 8, 8]             128
            ReLU-186             [-1, 64, 8, 8]               0
   ResBasicBlock-187             [-1, 64, 8, 8]               0
          Conv2d-188             [-1, 60, 8, 8]          34,560
     BatchNorm2d-189             [-1, 60, 8, 8]             120
            ReLU-190             [-1, 60, 8, 8]               0
          Conv2d-191             [-1, 64, 8, 8]          34,560
     BatchNorm2d-192             [-1, 64, 8, 8]             128
            ReLU-193             [-1, 64, 8, 8]               0
   ResBasicBlock-194             [-1, 64, 8, 8]               0
AdaptiveAvgPool2d-195             [-1, 64, 1, 1]               0
          Linear-196                   [-1, 10]             650
================================================================
Total params: 799,900
Trainable params: 799,900
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 13.88
Params size (MB): 3.05
Estimated Total Size (MB): 16.94
----------------------------------------------------------------
